{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fDSLUybycps"
   },
   "source": [
    "# HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ytgsqaTqCDaP"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime as dt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hassan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhQz0DdyiLM"
   },
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dGPIypTSwsgB"
   },
   "outputs": [],
   "source": [
    "URL = \"https://myanimelist.net/topanime.php\"\n",
    "urls = [] # list for storing urls of all the anime\n",
    "\n",
    "def get_urls():\n",
    "    \n",
    "    \"\"\"get_urls() returns the list of the urls for each anime\"\"\"\n",
    "    \n",
    "    for lim in range(0, 20000, 50):\n",
    "        r = requests.get(URL, params={\"limit\": lim})\n",
    "\n",
    "        if r.status_code == 404: # in case page is inaccessable\n",
    "            print(\"Unfortunately, page {} is inaccessable. We're interrupting the operation and returning the pages found.\".format(lim))\n",
    "\n",
    "        soup = bs(r.content, 'html5lib')\n",
    "\n",
    "        for res in soup.find_all('a', class_='hoverinfo_trigger fl-l ml12 mr8'):\n",
    "            url = res['href']\n",
    "            if url not in urls:\n",
    "                urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4kEgZgTxw9qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls.txt loaded.\n"
     ]
    }
   ],
   "source": [
    "filename = 'urls.txt'\n",
    "\n",
    "if filename not in os.listdir(): # create file if not already created\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('\\n'.join(list(map(str, urls))))\n",
    "\n",
    "else: # load file\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        urls = f.read().split(\"\\n\")\n",
    "        print(\"urls.txt loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMvU9V98yTSd",
    "outputId": "d9fb7c8f-c5b0-4ce3-ad45-83d2493157c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19218\n"
     ]
    }
   ],
   "source": [
    "print(len(urls)) # number of urls loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ArJ-ftIym9c"
   },
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "54Ladj690pSb"
   },
   "outputs": [],
   "source": [
    "def crawl_animes(urls_):\n",
    "    \n",
    "    \"\"\"crawl_animes function fetches html of every anime found by the get_url() method. It then\n",
    "    saves them in an 'htmls' directory. Inside 'htmls' directory, it saves htmls wrt to the page folder\n",
    "    it belongs to with the fashion 'htmls/page_rank_i/article_j.html'. In order to avoid repeatedly\n",
    "    downloading the htmls file, a binary file named as 'counter' is created to start from where\n",
    "    we left off in case of any interruption.\"\"\"\n",
    "    \n",
    "    if 'counter' not in os.listdir(): # initialize counter in case not already created\n",
    "        start = 0\n",
    "    else:\n",
    "        with open('counter', 'rb') as c: # load counter\n",
    "            start = pickle.load(c) + 1\n",
    "    print(\"Starting from anime no. {}\".format(start))\n",
    "\n",
    "    for i in range(start, len(urls_)):\n",
    "        page_rank = str(int(np.floor(i/50)))\n",
    "        \n",
    "        if i%50 == 0 or f\"page_rank_{page_rank}\" not in os.listdir('./htmls'):\n",
    "            os.mkdir('htmls/page_rank_{}'.format(page_rank))\n",
    "\n",
    "        html = requests.get(urls_[i])\n",
    "        sleep = 20\n",
    "\n",
    "        while html.status_code != 200:\n",
    "            print(\"Waiting {} seconds as we reach request limit while retrieving page no. {}.\\n\".format(sleep, i))\n",
    "            html.close()\n",
    "            time.sleep(sleep)\n",
    "            html = requests.get(urls_[i])\n",
    "            sleep += 5\n",
    "\n",
    "        with open(\"htmls/page_rank_{}/article_{}.html\".format(page_rank, i), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html.text)\n",
    "\n",
    "        with open(\"counter\", \"wb\") as c:\n",
    "            pickle.dump(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W-Bylq8UyVyt"
   },
   "outputs": [],
   "source": [
    "if 'htmls' not in os.listdir():\n",
    "  os.mkdir('htmls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VkoyFqsH7eJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from anime no. 19218\n"
     ]
    }
   ],
   "source": [
    "crawl_animes(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages(i_, folder_name=\"anime_tsvs\"):\n",
    "    \n",
    "    \"\"\"This routine parses the htmls we downloaded and fetches the information we are required in the homework\n",
    "    and saves them in an article_i.tsv file inside anime_tsvs directory.\"\"\"\n",
    "    \n",
    "    print(\"Working on page {}\".format(i_))\n",
    "    page_rank = str(int(np.floor(i_/50)))\n",
    "    article_path = \"htmls/page_rank_{}/article_{}.html\".format(page_rank, i_)\n",
    "\n",
    "    with open(article_path, 'r', encoding='utf-8') as f:\n",
    "        article = bs(f.read(), 'html.parser')\n",
    "\n",
    "    animeTitle = article.find(\"h1\", {\"class\":\"title-name h1_bold_none\"}).string\n",
    "    # print(animeTitle)\n",
    "\n",
    "    animeType = article.find(\"span\", {\"class\":\"information type\"}).string\n",
    "    # print(animeType)\n",
    "\n",
    "    contents = article.find_all('div', {'class': \"spaceit_pad\"})        \n",
    "    for c in contents:\n",
    "        span_ = c.find('span', {'class': \"dark_text\"})\n",
    "        if span_ is not None:\n",
    "            if span_.string == \"Episodes:\":\n",
    "                if c.contents[2] != '\\n  Unknown\\n  ':\n",
    "                    animeNumEpisode = int(c.contents[2])\n",
    "                else:\n",
    "                    animeNumEpisode = ''       \n",
    "    #             print(animeNumEpisode)\n",
    "\n",
    "            if span_.string == \"Aired:\":\n",
    "                dates_ = c.contents[2].string.replace('\\n', '').strip().split(' to ')\n",
    "    #             print(dates_)\n",
    "                if dates_[0] == 'Not available':\n",
    "                    releaseDate = ''\n",
    "                    endDate = ''\n",
    "                else:\n",
    "                    if len(dates_) == 2 and '?' not in dates_:                \n",
    "                        releaseDate = dates_[0]\n",
    "                        endDate = dates_[1]\n",
    "\n",
    "                        if len(releaseDate.split(' ')) == 3:\n",
    "                            releaseDate = dt.datetime.strptime(releaseDate, \"%b %d, %Y\") # Datetime conversion\n",
    "\n",
    "                        elif len(releaseDate.split(' ')) == 2:\n",
    "                            releaseDate = dt.datetime.strptime(releaseDate, \"%b %Y\")\n",
    "\n",
    "                        else:\n",
    "                            releaseDate = print(dt.datetime.strptime(releaseDate, \"%Y\"))\n",
    "\n",
    "                        if len(endDate.split(' ')) == 3:\n",
    "                            endDate = dt.datetime.strptime(endDate, \"%b %d, %Y\")\n",
    "\n",
    "                        elif len(endDate.split(' ')) == 2:\n",
    "                            endDate = dt.datetime.strptime(endDate, \"%b %Y\")\n",
    "\n",
    "                        else:\n",
    "                            endDate = dt.datetime.strptime(endDate, \"%Y\")\n",
    "                    else:\n",
    "                        endDate = ''\n",
    "                        releaseDate = dates_[0]\n",
    "\n",
    "                        if len(releaseDate.split(' ')) == 3:\n",
    "                            releaseDate = dt.datetime.strptime(releaseDate, \"%b %d, %Y\")\n",
    "\n",
    "                        elif len(releaseDate.split(' ')) == 2:\n",
    "                            releaseDate = dt.datetime.strptime(releaseDate, \"%b %Y\")\n",
    "\n",
    "                        else:\n",
    "                            releaseDate = dt.datetime.strptime(releaseDate, \"%Y\")\n",
    "\n",
    "    animeNumMembers = int(article.find(\"span\", {\"class\": \"numbers members\"}).contents[1].string.replace(',', ''))\n",
    "    # print(animeNumMembers)\n",
    "\n",
    "    if article.find(\"div\", {\"class\": \"score-label score-9\"}) is not None:\n",
    "        animeScore = float(article.find(\"div\", {\"class\": \"score-label score-9\"}).contents[0])\n",
    "    else:\n",
    "        animeScore = ''\n",
    "    # print(animeScore)\n",
    "\n",
    "    if article.find(\"span\", {\"itemprop\": {\"ratingCount\"}}) is not None:\n",
    "        animeUsers = int(article.find(\"span\", {\"itemprop\": {\"ratingCount\"}}).contents[0])\n",
    "    else:\n",
    "        animeUsers = ''\n",
    "    # print(animeUsers)\n",
    "\n",
    "    if (article.find(\"span\", {\"class\": \"numbers ranked\"}) is not None):\n",
    "        try:\n",
    "            animeRank = int(article.find(\"span\", {\"class\": \"numbers ranked\"}).contents[1].string[1:])\n",
    "        except:\n",
    "            animeRank = ''\n",
    "    else:\n",
    "        animeRank = ''\n",
    "    # print(animeRank)\n",
    "\n",
    "    if article.find(\"span\", {\"class\": \"numbers popularity\"}) is not None:\n",
    "        animePopularity = int(article.find(\"span\", {\"class\": \"numbers popularity\"}).contents[1].string[1:])\n",
    "    else:\n",
    "        animePopularity = ''\n",
    "    # print(animePopularity)\n",
    "\n",
    "    if article.find(\"p\", {\"itemprop\": {\"description\"}}) is not None:\n",
    "        animeDescription = article.find(\"p\", {\"itemprop\": {\"description\"}}).contents[0]\n",
    "    else:\n",
    "        animeDescription = ''\n",
    "    # print(animeDescription)\n",
    "\n",
    "    animeRelated = []\n",
    "\n",
    "    tbl_anime = article.find(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    if tbl_anime is not None:\n",
    "        anime_links = tbl_anime.find_all(\"a\")\n",
    "        for e in anime_links:\n",
    "            animeRelated.append(str(e.text))\n",
    "\n",
    "        animeRelated = list(set(animeRelated))\n",
    "        if '' in animeRelated:\n",
    "            animeRelated.remove('')\n",
    "        if ' ' in animeRelated:\n",
    "            animeRelated.remove(' ')\n",
    "    else:\n",
    "        animeRelated = ''\n",
    "    # print(animeRelated)\n",
    "\n",
    "    animeCharacters = []\n",
    "\n",
    "    tbl_characters = article.find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "    if tbl_characters is not None:\n",
    "        for e in tbl_characters:\n",
    "            a_ = e.find(\"a\")\n",
    "            animeCharacters.append((a_.text))\n",
    "    else:\n",
    "        animeCharacters = ''\n",
    "    # print(animeCharacters)\n",
    "\n",
    "    animeVoices = []\n",
    "\n",
    "    tbl_voices = article.find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "    if tbl_voices is not None:\n",
    "        for e in tbl_voices:\n",
    "            a_ = e.find(\"a\")\n",
    "            animeVoices.append((a_.text))\n",
    "    else:\n",
    "        animeVoices = ''\n",
    "\n",
    "    # print(animeVoices)\n",
    "\n",
    "    animeStaff = []\n",
    "    \n",
    "    if len(article.find_all('div', {'class': \"detail-characters-list clearfix\"})) > 1:\n",
    "        staff = article.find_all('div', {'class': \"detail-characters-list clearfix\"})[1]\n",
    "        td = staff.find_all('td', {'class': \"borderClass\"})\n",
    "    \n",
    "        for td_ in td:\n",
    "            if td_.get('width') == None:\n",
    "                animeStaff.append([td_.find('a').string, td_.find('small').string])\n",
    "    else:\n",
    "        animeStaff = ''\n",
    "    \n",
    "#     print(animeStaff)\n",
    "\n",
    "    with open('{}/anime_{}.tsv'.format(folder_name, i_), 'wt', e # save parsed info. into a tsv file\n",
    "              ncoding=\"utf8\") as f_:\n",
    "        tsv_wt = csv.writer(f_, delimiter='\\t')\n",
    "        tsv_wt.writerow([animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers,animeScore, \\\n",
    "                         animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, \\\n",
    "                         animeVoices, animeStaff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"anime_tsvs\" not in os.listdir():\n",
    "    os.mkdir(\"anime_tsvs\")\n",
    "    for i in range(len(urls)):\n",
    "        parse_pages(i)\n",
    "        \n",
    "for i in range(len(urls)):\n",
    "    parse_pages(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps that follow involves the merging of all the tsv, resulting in a dataframe. We then process this dataframe by working on its description (synopsis) field. We do tokenization, removing of stopwords & punctuation, and stemming. The resulting dataframe is saved in the csv format and in binary format for its use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_files(t):\n",
    "\n",
    "    \"\"\"This method sorts all the tsv files in the following fashion\n",
    "    anime_0.tsv, anime_1.tsv, anime_2.tsv, anime_3.tsv, .....\"\"\"\n",
    "\n",
    "    return [a(x) for x in re.split(r'(\\d+)', t)]\n",
    "\n",
    "def a(t):\n",
    "    return int(t) if t.isdigit() else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsvs(path, column_names):\n",
    "    \n",
    "    \"\"\"Here we merge the tsv files into a single dataframe.\"\"\"\n",
    "\n",
    "    list_of_files = sorted(os.listdir(path), key=sort_files)\n",
    "    df = pd.read_csv(path+list_of_files[0],\n",
    "                     names=column_names,\n",
    "                     sep=\"\\t\", engine='c')\n",
    "    \n",
    "    for f in list_of_files[1:]:\n",
    "        df_ = pd.read_csv(path+f,\n",
    "                          names=column_names,\n",
    "                          sep=\"\\t\", engine='c')\n",
    "        df = pd.concat([df, df_], ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./anime_tsvs/\"\n",
    "columns = [\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\", \"animeNumMembers\",\n",
    "            \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\", \"animeDescription\", \"animeRelated\",\n",
    "            \"animeCharacters\", \"animeVoices\", \"animeStaff\"]\n",
    "\n",
    "if \"df.csv\" not in os.listdir(): # then create and pre-process dataset\n",
    "    df = merge_tsvs(path, columns)\n",
    "    df = df.drop([0], axis=0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"animeNumMembers\"].fillna(0)\n",
    "    df[\"animePopularity\"].fillna(0)\n",
    "    df[\"animeNumMembers\"] = df[\"animeNumMembers\"].astype(int)\n",
    "    df[\"animePopularity\"] = df[\"animePopularity\"].astype(int)\n",
    "\n",
    "    df.to_csv(\"./df.csv\")\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text_, type_stemmer=\"porter\"): # we use porter stemmer by default\n",
    "\n",
    "    \"\"\"Here we process the synopsis as mentioned above. We return a list containing words which are\n",
    "    stemmed, tokenized, removed fom punctuation and stopwords.\"\"\"\n",
    "\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "\n",
    "    if type_stemmer == \"porter\":\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "    elif type_stemmer == \"lancaster\":\n",
    "        stemmer = nltk.stem.LancasterStemmer()\n",
    "        \n",
    "    try:\n",
    "        text_tokenized = nltk.word_tokenize(text_) # tokenization\n",
    "        stemmed = [stemmer.stem(word) for word in text_tokenized if ((word.lower() not in stopwords_english) and (word not in string.punctuation))] # stemming\n",
    "    except TypeError as e:\n",
    "        print(text_)\n",
    "        raise e\n",
    "        \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create (if not already) the dataframe with an additional column of preprocessed description\n",
    "\n",
    "if \"tokenized_df.p\" not in os.listdir():\n",
    "    df_tokenized = df.assign(description_tokenized=df[\"animeDescription\"].fillna('').apply(lambda m: text_process(m)))\n",
    "    with open(\"tokenized_df.p\", \"wb\") as f:\n",
    "        pickle.dump(df_tokenized, f)\n",
    "else:\n",
    "    with open(\"tokenized_df.p\", \"rb\") as f:\n",
    "        df_tokenized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(synopsis, vocabulary_file = \"vocabulary.pkl\"):\n",
    "    \n",
    "    \"\"\"Here we generate a vocab of all words from the description. We tag each word with an integer term_id\n",
    "    and then save it in a binary file.\"\"\"\n",
    "\n",
    "    vocab = set()\n",
    "\n",
    "    for desc in synopsis:\n",
    "        vocab = vocab.union(set(desc))\n",
    "\n",
    "    vocab_dict = dict(zip(sorted(vocab), range(len(vocab))))\n",
    "    with open(vocabulary_file, \"wb\") as f:\n",
    "        pickle.dump(vocab_dict, f)\n",
    "        \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_idx(synopsis, vocab, inverted_idx_file):\n",
    "    \n",
    "    \"\"\"Here we create a dictionary (inverted index) in which against each term id we have a list of documents no.\n",
    "    which contain that specific word.\"\"\"\n",
    "\n",
    "    inverted_idx = dict()\n",
    "    for term, term_id in vocab.items():\n",
    "        inverted_idx[term_id] = set() # create and initialize the dictionary with a set against each key to avoid duplicates\n",
    "\n",
    "    descriptions = zip(synopsis, range(len(synopsis)))   # tokenized description against doc no. \n",
    "    for desc, doc_n in descriptions:\n",
    "        checked_words = []\n",
    "        for word in desc:\n",
    "            if word not in checked_words: # check if we have already worked on this word\n",
    "                checked_words.append(word)\n",
    "                term_id = vocab[word]\n",
    "                inverted_idx[term_id] = inverted_idx[term_id].union(set([doc_n]))\n",
    "\n",
    "    for term_id, docs_set in inverted_idx.items():\n",
    "        inverted_idx[term_id] = sorted(list(inverted_idx[term_id]))\n",
    "\n",
    "    # create and save the inv_idx in a binary file\n",
    "    with open(inverted_idx_file, \"wb\") as f:\n",
    "        pickle.dump(inverted_idx, f)\n",
    "\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synopsis(synopsis_file = \"tokenized_df.p\"):\n",
    "\n",
    "    \"\"\"Here we load the descriptions.\"\"\"\n",
    "\n",
    "    print('Loading synopsis... ', end ='')\n",
    "    with open(synopsis_file, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    synopsis = list(df['description_tokenized'])\n",
    "    print('\\nSuccessfully loaded.\\n')\n",
    "    return synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(synopsis, vocabulary_file = \"vocabulary.pkl\"):\n",
    "    \n",
    "    \"\"\"Load vocabulary (in case it's present) otherwise create it.\"\"\"\n",
    "\n",
    "    print('Loading vocabulary... ', end ='')\n",
    "    if vocabulary_file not in os.listdir():\n",
    "        vocab = get_vocabulary(synopsis, vocabulary_file)\n",
    "    else:\n",
    "        with open(vocabulary_file, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "    print('\\nSuccessfully loaded.\\n')\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_idx(synopsis, vocab, inverted_idx_file = \"inverted_index.pkl\"):\n",
    "    \n",
    "    \"\"\"Load inverted index (in case it's present) otherwise create it.\"\"\"\n",
    "\n",
    "    print('Loading inverted index... ', end ='')\n",
    "    if inverted_idx_file not in os.listdir():\n",
    "        inverted_idx = inverted_idx(synopsis, vocab, inverted_idx_file)\n",
    "    else:\n",
    "        with open(inverted_idx_file, \"rb\") as f:\n",
    "            inverted_idx = pickle.load(f)\n",
    "    print('\\nSuccessfully loaded.\\n')\n",
    "    \n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading synopsis... \n",
      "Successfully loaded.\n",
      "\n",
      "Loading vocabulary... \n",
      "Successfully loaded.\n",
      "\n",
      "Loading inverted index... \n",
      "Successfully loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_file = \"vocabulary.pkl\"\n",
    "synopsis_file = \"tokenized_df.p\"\n",
    "inverted_idx_file = \"inverted_index.pkl\"\n",
    "\n",
    "# Load synopsis, vocabulary, and inverted index\n",
    "synopsis = get_synopsis(synopsis_file)\n",
    "vocab = get_vocab(synopsis, vocabulary_file)\n",
    "inverted_idx = get_inverted_idx(synopsis, vocab, inverted_idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(vocab, inverted_idx, urls):\n",
    "    \n",
    "    \"\"\"Search engine receives an input query and gives back the result of all anime documents that contain\n",
    "    every word of the query inputted.\"\"\"\n",
    "\n",
    "    query = input('Please enter your query...\\nquery: ') # Input query here\n",
    "\n",
    "    q = query.lower()\n",
    "    query = text_process(q) # pre-processing step\n",
    "\n",
    "    # if first word not in our vocab, then no need to search for later words (since it's an AND query)\n",
    "    if query[0] in vocab:\n",
    "        term_id_1 = vocab[query[0]]\n",
    "        docs_set = set(inverted_idx[term_id_1])\n",
    "\n",
    "        for word in query[1:]:\n",
    "            if word in vocab:\n",
    "                term_id = vocab[word]\n",
    "                docs = inverted_idx[term_id]\n",
    "\n",
    "                # Intersection is necassary to ensure all words of the query are in the synopsis\n",
    "                docs_set = docs_set.intersection(set(docs))\n",
    "\n",
    "                # In case no intersection found\n",
    "                if len(docs_set) == 0:\n",
    "                    print(\"No result found.\")\n",
    "                    return\n",
    "\n",
    "            else:\n",
    "                print(\"No result found.\")\n",
    "                return\n",
    "\n",
    "        df = pd.read_csv(\"./df.csv\") # df containing the processed snypsis\n",
    "        \n",
    "        res = df.iloc[sorted(list(docs_set))][[\"animeTitle\", \"animeDescription\"]]\n",
    "        \n",
    "        for i in sorted(list(docs_set)):\n",
    "            res['URL'] = urls[i]\n",
    "\n",
    "        return res\n",
    "\n",
    "    else:\n",
    "        print('No result found.')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query...\n",
      "query: saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "367                                       Dragon Ball Z   \n",
       "402                            Dragon Ball Super: Broly   \n",
       "1470  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "367   Five years after winning the World Martial Art...   \n",
       "402   Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1470  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "\n",
       "                                                    URL  \n",
       "367   https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "402   https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "1470  https://myanimelist.net/anime/986/Dragon_Ball_...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine(vocab, inverted_idx, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf(word, desc, synopsis, idf=None):\n",
    "    \n",
    "    \"\"\"Here we calculate tfidf score corresponding the inputted word.\"\"\"\n",
    "\n",
    "    counter = 0\n",
    "    if idf == None: # calculate idf if not provided\n",
    "        for desc in synopsis:\n",
    "            if word in desc:\n",
    "                counter += 1\n",
    "                \n",
    "        idf = np.log(len(synopsis)/counter)\n",
    "        \n",
    "    tfidf = desc.count(word)/len(desc) * idf\n",
    "    \n",
    "    return idf, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_idx_2(synopsis, vocab, inverted_idx_tfidf_file=\"inverted_index_2.p\", idfs_file=\"idfs.p\"):\n",
    "    \n",
    "    \"\"\"Here we generate a dictionary for our inverted index \"\"\"\n",
    "    \n",
    "    second_inverted_idx = dict()\n",
    "    for term_id in vocab.values():\n",
    "        second_inverted_idx[term_id] = list()\n",
    "\n",
    "    calculated_idfs = {}\n",
    "    \n",
    "    descriptions = zip(synopsis, range(len(synopsis)))\n",
    "    for desc, doc_n in descriptions:\n",
    "        checked_words = []\n",
    "        for word in desc:\n",
    "            # avoid redundancy of checking already checked words\n",
    "            if word not in checked_words:\n",
    "                checked_words.append(word)\n",
    "                term_id = vocab[word]\n",
    "                \n",
    "                if word not in calculated_idfs.keys():\n",
    "                    idf, tfidf = find_tfidf(word, desc, synopsis) # calculate idf and tfidf for this new word\n",
    "                    calculated_idfs[word] = idf\n",
    "                    \n",
    "                else:\n",
    "                    _, tfidf = find_tfidf(word, desc, synopsis, idf)\n",
    "\n",
    "                second_inverted_idx[term_id].append([doc_n, tfidf]) # append document id and corresponding tfidf score\n",
    "\n",
    "    for term_id, lists in second_inverted_idx.items():\n",
    "        second_inverted_idx[term_id] = sorted(second_inverted_idx[term_id], key=lambda m: m[1]) # sort by tfidf score\n",
    "\n",
    "    with open(inverted_idx_tfidf_file, \"wb\") as f:\n",
    "        pickle.dump(second_inverted_idx, f)\n",
    "\n",
    "    with open(idfs_file, \"wb\") as f:\n",
    "        pickle.dump(calculated_idfs, f)\n",
    "\n",
    "    return second_inverted_idx, calculated_idfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_idx_tfidf(synopsis, vocab, inverted_idx_tfidf_file, idfs_file):\n",
    "\n",
    "    \"\"\"Load inverted index with tfidfs (in case it's present) otherwise create it.\"\"\"\n",
    "\n",
    "    print('Loading inverted index tfidf... \\n', end ='')\n",
    "    if (idfs_file not in os.listdir()) or (inverted_idx_tfidf_file not in os.listdir()):\n",
    "        inv_idx_2, idfs = inverted_idx_2(synopsis, vocab, inverted_idx_tfidf_file, idfs_file)\n",
    "        \n",
    "    else:\n",
    "        with open(inverted_idx_tfidf_file, \"rb\") as f:\n",
    "            inv_idx_2 = pickle.load(f)\n",
    "            \n",
    "        with open(idfs_file, \"rb\") as f:\n",
    "            idfs = pickle.load(f)\n",
    "    print('Successfully loaded.')\n",
    "    return inv_idx_2, idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inverted index tfidf... \n",
      "Successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "inverted_idx_tfidf_file = \"inverted_index_2.p\"\n",
    "idfs_file = \"idfs.p\"\n",
    "\n",
    "inv_idx_2, idfs = get_inverted_idx_tfidf(synopsis, vocab, inverted_idx_tfidf_file, idfs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cos_similarity(vector_1, vector_2):\n",
    "\n",
    "    \"\"\"Computes cosine similarity between two vectors\"\"\"\n",
    "    \n",
    "    return (np.dot(vector_1, vector_2))/(np.linalg.norm(vector_1) * np.linalg.norm(vector_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_docs(query, synopsis, vocab, inv_idx_2, idfs, urls, k=10):\n",
    "\n",
    "    \"\"\"Here we create max-heap of the documents containing words of the input query,\n",
    "    we then arrange them wrt cosine similarity of these documents with the query and\n",
    "    return top k documents only.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(\"./df.csv\")\n",
    "\n",
    "    query = text_process(query.lower()) # query pre-processing\n",
    "\n",
    "    res_dict = {} # result dictionary\n",
    "\n",
    "    for word in query:\n",
    "        if word in vocab.keys():\n",
    "            term_id = vocab[word]\n",
    "            for list_ in inv_idx_2[term_id]:\n",
    "                if list_[0] not in res_dict.keys():\n",
    "                    res_dict[list_[0]] = []\n",
    "                res_dict[list_[0]].append(list_[1])\n",
    "#         else:\n",
    "#             print(\"No result found.\")\n",
    "\n",
    "    vector_query = [(query.count(q)/len(query)) * idfs[q] for q in query if q in idfs.keys()]\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for key in res_dict.keys():\n",
    "        vec = res_dict[key]\n",
    "        if len(vec) == len(vector_query):\n",
    "            dists.append((-find_cos_similarity(vector_query, vec), key))\n",
    "\n",
    "    heapq.heapify(dists) # using heap data structure\n",
    "    dists_len = len(dists)\n",
    "    res = []\n",
    "    for i in range(min(k, dists_len)):\n",
    "        e = heapq.heappop(dists)\n",
    "        res.append([e[1], -e[0]])\n",
    "\n",
    "    indices = [i[0] for i in res]\n",
    "    dists = [i[1] for i in res]\n",
    "\n",
    "    df_1 = df.iloc[indices][[\"animeTitle\", \"animeDescription\"]]\n",
    "    \n",
    "    df_res = df_1.assign(URL=[urls[i] for i in indices],\n",
    "                                 Similarity=dists)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>URL</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>Oshiete! Galko-chan</td>\n",
       "      <td>At first glance, Galko, Otako, and Ojou are th...</td>\n",
       "      <td>https://myanimelist.net/anime/32013/Oshiete_Ga...</td>\n",
       "      <td>0.999756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>Astarotte no Omocha! EX</td>\n",
       "      <td>The OVA Astaroette no Omocha is a three part s...</td>\n",
       "      <td>https://myanimelist.net/anime/10582/Astarotte_...</td>\n",
       "      <td>0.999431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16381</th>\n",
       "      <td>Ishiyama-dera Digital Engi Emaki</td>\n",
       "      <td>The Ishiyama-dera temple in Outsu announced th...</td>\n",
       "      <td>https://myanimelist.net/anime/33982/Ishiyama-d...</td>\n",
       "      <td>0.999240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>Panda Kopanda</td>\n",
       "      <td>Panda Kopanda (Panda! Go Panda!) is a 30 minut...</td>\n",
       "      <td>https://myanimelist.net/anime/2611/Panda_Kopanda</td>\n",
       "      <td>0.999154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>Mousou Dairinin</td>\n",
       "      <td>The infamous Shounen Bat (Lil' Slugger) is ter...</td>\n",
       "      <td>https://myanimelist.net/anime/323/Mousou_Dairinin</td>\n",
       "      <td>0.998555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>Saa Ikou! Tamagotchi</td>\n",
       "      <td>Let's Go! Tamagotchi is an anime series focusi...</td>\n",
       "      <td>https://myanimelist.net/anime/6798/Saa_Ikou_Ta...</td>\n",
       "      <td>0.998269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>Touyama Sakura Uchuu Chou: Yatsu no Na wa Gold</td>\n",
       "      <td>This science-fiction anime was inspired somewh...</td>\n",
       "      <td>https://myanimelist.net/anime/12399/Touyama_Sa...</td>\n",
       "      <td>0.996559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>Sakurako-san no Ashimoto ni wa Shitai ga Umatt...</td>\n",
       "      <td>When Shoutarou Tatewaki first meets Sakurako K...</td>\n",
       "      <td>https://myanimelist.net/anime/30187/Sakurako-s...</td>\n",
       "      <td>0.995053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9358</th>\n",
       "      <td>Metropolis (2009)</td>\n",
       "      <td>Mirai Mizue's first time experimenting with ge...</td>\n",
       "      <td>https://myanimelist.net/anime/29765/Metropolis...</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>One Piece: Episode of Merry - Mou Hitori no Na...</td>\n",
       "      <td>The story arcs aboard the Straw Hat Crew's fir...</td>\n",
       "      <td>https://myanimelist.net/anime/19123/One_Piece_...</td>\n",
       "      <td>0.992909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              animeTitle  \\\n",
       "3430                                 Oshiete! Galko-chan   \n",
       "5559                             Astarotte no Omocha! EX   \n",
       "16381                   Ishiyama-dera Digital Engi Emaki   \n",
       "4844                                       Panda Kopanda   \n",
       "1119                                     Mousou Dairinin   \n",
       "9509                                Saa Ikou! Tamagotchi   \n",
       "10786     Touyama Sakura Uchuu Chou: Yatsu no Na wa Gold   \n",
       "1907   Sakurako-san no Ashimoto ni wa Shitai ga Umatt...   \n",
       "9358                                   Metropolis (2009)   \n",
       "325    One Piece: Episode of Merry - Mou Hitori no Na...   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "3430   At first glance, Galko, Otako, and Ojou are th...   \n",
       "5559   The OVA Astaroette no Omocha is a three part s...   \n",
       "16381  The Ishiyama-dera temple in Outsu announced th...   \n",
       "4844   Panda Kopanda (Panda! Go Panda!) is a 30 minut...   \n",
       "1119   The infamous Shounen Bat (Lil' Slugger) is ter...   \n",
       "9509   Let's Go! Tamagotchi is an anime series focusi...   \n",
       "10786  This science-fiction anime was inspired somewh...   \n",
       "1907   When Shoutarou Tatewaki first meets Sakurako K...   \n",
       "9358   Mirai Mizue's first time experimenting with ge...   \n",
       "325    The story arcs aboard the Straw Hat Crew's fir...   \n",
       "\n",
       "                                                     URL  Similarity  \n",
       "3430   https://myanimelist.net/anime/32013/Oshiete_Ga...    0.999756  \n",
       "5559   https://myanimelist.net/anime/10582/Astarotte_...    0.999431  \n",
       "16381  https://myanimelist.net/anime/33982/Ishiyama-d...    0.999240  \n",
       "4844    https://myanimelist.net/anime/2611/Panda_Kopanda    0.999154  \n",
       "1119   https://myanimelist.net/anime/323/Mousou_Dairinin    0.998555  \n",
       "9509   https://myanimelist.net/anime/6798/Saa_Ikou_Ta...    0.998269  \n",
       "10786  https://myanimelist.net/anime/12399/Touyama_Sa...    0.996559  \n",
       "1907   https://myanimelist.net/anime/30187/Sakurako-s...    0.995053  \n",
       "9358   https://myanimelist.net/anime/29765/Metropolis...    0.994400  \n",
       "325    https://myanimelist.net/anime/19123/One_Piece_...    0.992909  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"first anime\"\n",
    "output = find_top_k_docs(query, synopsis, vocab, inv_idx_2, idfs, urls)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>URL</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>Manga Sekai Mukashibanashi</td>\n",
       "      <td>Each episode of this series tells the story of...</td>\n",
       "      <td>https://myanimelist.net/anime/6262/Manga_Sekai...</td>\n",
       "      <td>0.998960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8277</th>\n",
       "      <td>Kuusou no Sora Tobu Kikaitachi</td>\n",
       "      <td>Animated 2002 short film produced by Studio Gh...</td>\n",
       "      <td>https://myanimelist.net/anime/19401/Kuusou_no_...</td>\n",
       "      <td>0.971399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>Glass no Kamen</td>\n",
       "      <td>Kitajima Maya, a 13-year old girl with a talen...</td>\n",
       "      <td>https://myanimelist.net/anime/506/Glass_no_Kamen</td>\n",
       "      <td>0.969861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>Chou Hatsumei Boy Kanipan</td>\n",
       "      <td>Cho Hatsumei Boy Kanipan is a continued story ...</td>\n",
       "      <td>https://myanimelist.net/anime/3690/Chou_Hatsum...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>Marginal Prince: Gekkeiju no Ouji-tachi</td>\n",
       "      <td>\"Marginal Prince\" based off of the hit love si...</td>\n",
       "      <td>https://myanimelist.net/anime/1912/Marginal_Pr...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14808</th>\n",
       "      <td>Abe George Kattobi Seishun Ki: Shibuya Honky Tonk</td>\n",
       "      <td>This story is based on one of the famous Japan...</td>\n",
       "      <td>https://myanimelist.net/anime/17501/Abe_George...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16923</th>\n",
       "      <td>Kono Shihai kara no Sotsugyou: Ozaki Yutaka</td>\n",
       "      <td>A compilation OVA of 5 stories portraying the ...</td>\n",
       "      <td>https://myanimelist.net/anime/12981/Kono_Shiha...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>Madou King Granzort</td>\n",
       "      <td>In the future, the moon is a habitable place w...</td>\n",
       "      <td>https://myanimelist.net/anime/2818/Madou_King_...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>Jin Sheng Yuan</td>\n",
       "      <td>A famous music video tells a sentimental story...</td>\n",
       "      <td>https://myanimelist.net/anime/10132/Jin_Sheng_...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>Sekai Meisaku Douwa</td>\n",
       "      <td>Another of Toei's World Famous Fairy Tale seri...</td>\n",
       "      <td>https://myanimelist.net/anime/7398/Sekai_Meisa...</td>\n",
       "      <td>0.962115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              animeTitle  \\\n",
       "3913                          Manga Sekai Mukashibanashi   \n",
       "8277                      Kuusou no Sora Tobu Kikaitachi   \n",
       "2276                                      Glass no Kamen   \n",
       "5665                           Chou Hatsumei Boy Kanipan   \n",
       "9253             Marginal Prince: Gekkeiju no Ouji-tachi   \n",
       "14808  Abe George Kattobi Seishun Ki: Shibuya Honky Tonk   \n",
       "16923        Kono Shihai kara no Sotsugyou: Ozaki Yutaka   \n",
       "3411                                 Madou King Granzort   \n",
       "7689                                      Jin Sheng Yuan   \n",
       "8237                                 Sekai Meisaku Douwa   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "3913   Each episode of this series tells the story of...   \n",
       "8277   Animated 2002 short film produced by Studio Gh...   \n",
       "2276   Kitajima Maya, a 13-year old girl with a talen...   \n",
       "5665   Cho Hatsumei Boy Kanipan is a continued story ...   \n",
       "9253   \"Marginal Prince\" based off of the hit love si...   \n",
       "14808  This story is based on one of the famous Japan...   \n",
       "16923  A compilation OVA of 5 stories portraying the ...   \n",
       "3411   In the future, the moon is a habitable place w...   \n",
       "7689   A famous music video tells a sentimental story...   \n",
       "8237   Another of Toei's World Famous Fairy Tale seri...   \n",
       "\n",
       "                                                     URL  Similarity  \n",
       "3913   https://myanimelist.net/anime/6262/Manga_Sekai...    0.998960  \n",
       "8277   https://myanimelist.net/anime/19401/Kuusou_no_...    0.971399  \n",
       "2276    https://myanimelist.net/anime/506/Glass_no_Kamen    0.969861  \n",
       "5665   https://myanimelist.net/anime/3690/Chou_Hatsum...    0.962115  \n",
       "9253   https://myanimelist.net/anime/1912/Marginal_Pr...    0.962115  \n",
       "14808  https://myanimelist.net/anime/17501/Abe_George...    0.962115  \n",
       "16923  https://myanimelist.net/anime/12981/Kono_Shiha...    0.962115  \n",
       "3411   https://myanimelist.net/anime/2818/Madou_King_...    0.962115  \n",
       "7689   https://myanimelist.net/anime/10132/Jin_Sheng_...    0.962115  \n",
       "8237   https://myanimelist.net/anime/7398/Sekai_Meisa...    0.962115  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"famous story\"\n",
    "output = find_top_k_docs(query, synopsis, vocab, inv_idx_2, idfs, urls)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow:\n",
    "1. Given an input list of appointments, find all combinations of the possible solutions\n",
    "2. Check each combination if it is valid or not (no consecutive appointments)\n",
    "3. For all valid combinations, find their durations\n",
    "4. Find the combination with the maximum duration\n",
    "5. Return list of the last step and its duration\n",
    "\n",
    "Input: appointments_list of length n and distinct values\\\n",
    "\n",
    "routine max_len_appointments(appointments_list):\\\n",
    "    validCombinations = [all combinations in which ther are no consecutive appointments]\\\n",
    "    appointmentDurations = [durations of every instance of validCombinations]\\\n",
    "    maxDuration = max(appointmentDurations)\\\n",
    "    maxLenappointments = [instances of appointmentDurations where duration is maxDuration]\n",
    "    \n",
    "    return maxLenappointments, maxDuration\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
